{"meta":{"title":"ye's blog","subtitle":"","description":"","author":"Ye","url":"https://Bayeeaa.github.io","root":"/"},"pages":[{"title":"about","date":"2023-08-22T07:06:19.000Z","updated":"2023-08-22T07:07:20.142Z","comments":false,"path":"about/index.html","permalink":"https://bayeeaa.github.io/about/index.html","excerpt":"","text":"hello!"}],"posts":[{"title":"爬取pixivic日前十图片并下载至本地","slug":"pixivic-crawler","date":"2023-09-11T04:21:25.000Z","updated":"2023-09-11T04:23:57.180Z","comments":true,"path":"2023/09/11/pixivic-crawler/","link":"","permalink":"https://bayeeaa.github.io/2023/09/11/pixivic-crawler/","excerpt":"","text":"爬取pixivic日前十图片实现目标：通过爬虫代码将pixivic日前十图片下载至本地并通过文件夹分类日期： 1.发送请求首先通过f12检查网站传输的包，发现ranks文件，查看其api所对应的响应，可以找到图片的url下载地址： 于是我们就可以向这个api发送请求： 1234import requestsurl = &quot;https://api.bbmang.me/ranks?page=1&amp;date=2023-09-07&amp;mode=day&amp;pageSize=30&quot;reponse = requests.get(url)print(reponse) #返回&lt;Response [200]&gt; 因为返回值为200，说明服务器已经同意请求，并通过reponse.text发现其返回值是标准的json文件，没有出现乱码等反爬现象，因此不用再加入请求头。 2.解析数据返回数据长这个样子： json使用这就是标准的json形式文件，但还不是一个字典(其实很想)，所以我们要通过response.json()指令将其变成一个字典，这样才可以通过字典与列表的操作方式提取数据。 我们可以看到里面的数据其实就是字典与列表的不断嵌套，所以只要一点点剥下去即可： 123response.json()[&quot;data&quot;][0][&#x27;imageUrls&#x27;][0][&#x27;original&#x27;]#返回为https://i.pximg.net/img-original/img/2023/09/06/00/46/45/111480211_p0.png#就是日排行第一的图 但是当你将网站复制进浏览器时你会发现打不开。 url拼接也许是因为这个url是p站之前的使用方法，这时通过与现在图片的打开地址进行比对可以看到其中可以进行暴力拼接 图中方框处就是相同的部分，因此我们可以通过列表的切片将其拼接，这样我们就可以得到可以用的图片下载url了。 3.下载图片至本地目前我们已经得到了今天top1的图片url，我们再通过request请求获取图片数据，然后就可以把图片下载到本地了： 12345img_url = &quot;https://acgpic.net/c/540x540_70/img-master&quot;+response.json()[&quot;data&quot;][0][&#x27;imageUrls&#x27;][0][&#x27;original&#x27;][32:-4]+&quot;_master1200.jpg&quot;img = requests.get(img_url)with open(&quot;./top1.jpg&quot;,&quot;wb&quot;) as f: #&quot;wb&quot;是以二进制写入 f.write(img.content) #content是指获取二进制内容 此时你已经可以看到top1的图片下载当前文件夹了。 同理，下载前十的图片只需要加入for循环就可以了，以下省略。 4.下载至指定文件夹这里就需要导入os包： 1234import ospath = &#x27;C:/Users/yyn19/Desktop/code/download_imges/2023-9-7&#x27; #这是我的绝对地址if(os.path.exists(path)==False): #判断是否存在该文件夹 os.mkdir(path) #创建该文件夹，文件夹名字为2023-9-7 path书写这其中的path是linux写法书写，为了方便我进行字符串拼接，这其中path也有三种写法： 第二条中的r是用来申明不是转义字符。 这样就可以创建一个文件夹了~ 然后只要再with open() as中加入path路径就可以将其下载到指定文件夹中了。 5.关于api中日期的变化因为每天api的地址都会更新，这里我就用了datetime包来获取时间信息，我发现api中的日期都会比我们现在的时间少3天，因此只要一下操作就能获取api中的时间日期： 123import datetimeday = datetime.date.today()api_day = day-datetime.timedelta(days=3) 然后只需要再拼接如url中就可以啦~ 完整代码123456789101112131415161718192021import requestsimport osimport datetimeday = datetime.date.today()api_day = day-datetime.timedelta(days=3)url = &quot;https://api.bbmang.me/ranks?page=1&amp;date=&quot;+str(api_day)+&quot;&amp;mode=day&amp;pageSize=302&quot;response = requests.get(url)date = url[40:50]path = &#x27;C:/Users/yyn19/Desktop/code/download_imges/&#x27;+date #linux写法if(os.path.exists(path)==False): #判断是否存在该文件夹 os.mkdir(path) #创建该文件夹for i in range(10): img_url = &quot;https://acgpic.net/c/540x540_70/img-master&quot;+response.json()[&quot;data&quot;][i][&#x27;imageUrls&#x27;][0][&#x27;original&#x27;][32:-4]+&quot;_master1200.jpg&quot; img = requests.get(img_url) with open(path+&quot;/top&quot;+str(i+1)+&quot;.jpg&quot;,&quot;wb&quot;) as f: f.write(img.content) print(&quot;正在保存top&quot;+str(i+1)+&quot;中...&quot;)","categories":[],"tags":[]},{"title":"爬虫 --- 以爬取笔趣阁小说为例","slug":"biquge","date":"2023-08-31T07:40:23.000Z","updated":"2023-09-10T14:03:51.640Z","comments":true,"path":"2023/08/31/biquge/","link":"","permalink":"https://bayeeaa.github.io/2023/08/31/biquge/","excerpt":"","text":"爬虫 — 以爬取笔趣阁小说为例1.发送请求123456import requestsurl = &quot;https://www.xzmncy.com/list/5418/2610707.html&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot;&#125;response = requests.get(url,headers) 这是requests请求，若返回response值为200，则表示请求成功 2.获取数据1response = requests.get(url,headers).text 可以通过以上方法返回得到的html文件内容，而文件中有很多标签在里面，不能直接获取想要的信息，所以需要数据解析 3.解析数据有以下几种途径：css、xpath、re正则表达 等等 让我们来看看分别用这三种方法怎么去解析到一个章节的标题 css123import parselselector = parsel.Selector(response)novel_title = selector.css(&quot;.bookname h1::text&quot;).get() 这种方法通过css选择器进行选择 xpath123import parselselector = parsel.Selector(response)novel_title = selector.xpath(&quot;//*[@class=&quot;bookname&quot;]/h1/text()&quot;).get() 注意text后面的() re12import renovel_title = re.findall(&quot;&lt;h1&gt;(.*?)&lt;/h1&gt;&quot;,response)[0] 这里是因为h1在html文件中只有一个，故我直接导入。获取的数据是一个列表，所以我后面做了个且切片来直接获取字符串 *注意：以上方法各有利弊，选择合适的方式来解析数据 4.保存数据12with open(&quot;file_name&quot;+&quot;.txt&quot;,mode=&quot;w&quot;,encoding=&quot;utf-8&quot;) as f: #w是写入但是覆盖，a是追加写入，写入文件末尾 wb是二进制写入f.write(novel_context) #写入文件 with open(download_path,mode&#x3D;””,encoding&#x3D;”utf-8”)中间的download_path可以写绝对路径 以上思路已经理清楚了，下面开始实践：爬取一章12345678910111213import parselimport requestsurl = &quot;https://www.xzmncy.com/list/5418/2610707.html&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot;&#125;response = requests.get(url,headers).textselector = parsel.Selector(response)novel_title = selector.css(&quot;.bookname h1::text&quot;).get() #css方法解析数据novel_context_list = selector.css(&quot;#htmlContent p::text&quot;).getall() novel_context = &quot;\\n&quot;.join(novel_context_list) 注意：join函数的使用： 123456a=[&quot;1&quot;,&quot;2&quot;,&quot;8&quot;,&quot;9&quot;]print(&quot; &quot;.join(a)) #输出1 2 8 9print(&quot;\\n&quot;.join(a)) #输出1(换行)2(换行)8(换行)9b=&#123;&quot;a&quot;:1,&quot;b&quot;:2&#125;print(&quot; &quot;.join(a)) #输出a b （注意seq不能是int整形） 爬取各章url12345678910111213import requestsimport reurl = &quot;https://www.xzmncy.com/list/18753/&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot; &#125;response = requests.get(url,headers).textnovel_name = re.findall(&quot;&lt;h1&gt;(.*?)&lt;/h1&gt;&quot;,response)[0]novel_info = re.findall(&#x27;&lt;dd&gt;&lt;a href=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&lt;/dd&gt;&#x27;,response)for novel_url_part,novel_title in novel_info: novel_url = &quot;https://www.xzmncy.com&quot;+novel_url_part[0:24] print(novel_url) print(novel_title) 在小说的列表页面我们可以发现每个标签对应的章节url，此时我们获取的数据是这样的： 我们就可以用re来解析到各个章节的url和title 完整代码123456789101112131415161718192021222324import requestsimport reimport parselurl = &quot;https://www.xzmncy.com/list/18753/&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot; &#125;response = requests.get(url,headers).textnovel_name = re.findall(&quot;&lt;h1&gt;(.*?)&lt;/h1&gt;&quot;,response)[0]novel_info = re.findall(&#x27;&lt;dd&gt;&lt;a href=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&lt;/dd&gt;&#x27;,response)for novel_url_part,novel_title in novel_info: novel_url = &quot;https://www.xzmncy.com&quot;+novel_url_part[0:24] novel_response = requests.get(novel_url, headers).text selectors = parsel.Selector(novel_response) novel_context_list = selectors.css(&quot;#htmlContent p::text&quot;).getall() novel_context = &quot;\\n&quot;.join(novel_context_list) print(&quot;正在保存&quot;+novel_title) novel_title = &quot;*&quot; + novel_title with open(novel_name+&quot;.txt&quot;,mode=&quot;a&quot;) as f: f.write(novel_title) f.write(&quot;\\n&quot;) f.write(novel_context) f.write(&quot;\\n&quot;) f.write(&quot;\\n&quot;) 运行代码就可以看到当前的目录下出现一个txt文件，里面就是想要的小说啦~","categories":[],"tags":[]},{"title":"CSS选择器","slug":"css选择器","date":"2023-08-23T04:35:42.000Z","updated":"2023-08-24T07:48:46.105Z","comments":true,"path":"2023/08/23/css选择器/","link":"","permalink":"https://bayeeaa.github.io/2023/08/23/css%E9%80%89%E6%8B%A9%E5%99%A8/","excerpt":"","text":"引入方式引入方式有以下三种： 1.内嵌式 12345678910111213&lt;!-- 内嵌式 --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style&gt; colour&#123; colour:pink; &#125; &lt;/style&gt;&lt;/head&gt; 2.外联式 12345678910&lt;!-- 外联式 --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Document&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;./111.css&quot;&gt; &lt;!-- 111为引入文件名 --&gt;&lt;/head&gt; 3.行内式 1234567&lt;!-- 行内式 --&gt;&lt;body&gt; &lt;div class=&quot;colour&quot;&gt; abcd &lt;/div&gt; &lt;div style=&quot;color: aqua;font-size: large;&quot;&gt;abab&lt;/div&gt;&lt;/body&gt; 选择器一共有4种：标签选择器、类选择器、id选择器、通符选择器 注：一下选择器均是在style标签下的 1.标签123div&#123; color:blue;&#125; 所选类型与body中html的文本类型相符(如上文中，其下文body中应该是div) 2.类选择器123.color-choose&#123; color:blue;&#125; 其html调用方式为： 1&lt;div class=&quot;color-choose&quot;&gt; abab &lt;/div&gt; 3.id选择器123#color&#123; color:blue;&#125; 其html调用方式为： 1&lt;div id=&quot;color&quot;&gt; abab &lt;/div&gt; 注意：id只得调用一次 4.通符选择器12345*&#123; margin:0; padding:0;&#125;&lt;!-- 清除内外边距 --&gt; 对全局内容生效 选择器的选择1.后代 （后面所有代）问题如下 1234&lt;p&gt; abab &lt;/p&gt;&lt;div&gt; &lt;p&gt; 哈哈哈 &lt;/p&gt;&lt;/div&gt; 欲选择div中的p标签，而不是外部的p 以如下方法实现： 12345&lt;style&gt; div p &#123; color:blue; &#125;&lt;/style&gt; 2.子代 （后面一代）问题是要选中div后面的一代 123456&lt;div&gt; &lt;p&gt; dd &lt;/p&gt; &lt;p&gt; &lt;a href=&quot;#&quot;&gt; ddd &lt;/a&gt; &lt;/p&gt;&lt;/div&gt; 以如下方法实现： 123div&gt;p&#123; color:blue;&#125; 3.并集问题：想要让以下这些标签被选到 1234&lt;p&gt; p &lt;/p&gt;&lt;div&gt; div &lt;/div&gt;&lt;span&gt; span &lt;/span&gt;&lt;h1&gt; haha &lt;/h1&gt; 以下面方法实现： 123p,div,span,h1&#123; color:blue;&#125; 4.交集问题：只想要选中下面p中带class&#x3D;”c”的 1234&lt;div class=&quot;c&quot;&gt;abcd&lt;/div&gt;&lt;p class=&quot;c&quot;&gt;a&lt;/p&gt;&lt;div class=&quot;c&quot;&gt;d&lt;/div&gt;&lt;p class=&quot;c&quot;&gt;b&lt;/p&gt; 以下面方法实现： 123p.c&#123; color:blue;&#125; p是标签，c是类名（前面带个.的） 5.伪类问题：想要让鼠标悬停在如下超链接上能够变色 1&lt;a href=&quot;~~~&quot;&gt;传送&lt;/a&gt; 一下方法实现： 1234a:hover&#123; color:red; background-color:yellow;&#125;","categories":[],"tags":[]},{"title":"破壳啦","slug":"page1","date":"2023-08-21T11:25:42.000Z","updated":"2023-08-22T13:45:57.473Z","comments":true,"path":"2023/08/21/page1/","link":"","permalink":"https://bayeeaa.github.io/2023/08/21/page1/","excerpt":"","text":"终于创建好一个博客啦！","categories":[],"tags":[]}],"categories":[],"tags":[]}