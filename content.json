{"meta":{"title":"ye's blog","subtitle":"","description":"","author":"Ye","url":"https://Bayeeaa.github.io","root":"/"},"pages":[{"title":"about","date":"2023-08-22T07:06:19.000Z","updated":"2023-08-22T07:07:20.142Z","comments":false,"path":"about/index.html","permalink":"https://bayeeaa.github.io/about/index.html","excerpt":"","text":"hello!"}],"posts":[{"title":"递归——深度优先搜索(dfs)","slug":"递归——深度优先搜索(dfs)","date":"2023-11-06T14:26:27.000Z","updated":"2023-11-06T14:33:30.833Z","comments":true,"path":"2023/11/06/递归——深度优先搜索(dfs)/","link":"","permalink":"https://bayeeaa.github.io/2023/11/06/%E9%80%92%E5%BD%92%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2(dfs)/","excerpt":"","text":"递归——深度优先搜索(dfs)区别与广度优先(bfs)，深度优先注重的是一步走到底，通俗的举一个例子，比如一个迷宫，每走一格他就有很多的方向可以走，而深度优先就是先选取一个方向并且一路走到底直到触边或无路可走时再返回。以下使用递归方法实现深度优先搜索： 递归方法类似于栈，将数据一直递取到底后自下往上出栈。 大致框架如下： 1234567891011121314viod dfs(int k)&#123; if(输出条件)&#123; cout&lt;&lt; &#125; else&#123; for(int i=0;i&lt;n;i++)&#123; if(vis[i]==0)&#123; a[k]=数字,vis[i]=1//标记使用; dfs(k+1);//向下递取 vis[i]=0;//将其拿出，返回原先状态 &#125; &#125; &#125;&#125; 下列题目方式解决一些排列组合问题。 组合输出 –5个数字组合输入3个盒子 1234567891011121314151617181920212223242526272829303132333435#include&lt;bits/stdc++.h&gt;using namespace std;int m,n,r;int a[200],vis[200];//a用来记录牌子，vis用来记录牌子的使用情况int is_rise(int b[])&#123;//判断是否是递增数组 int flag=1; for(int i=1;i&lt;r;i++)&#123; if(a[i]&gt;a[i+1])&#123; flag=0; &#125; &#125; return flag;&#125;void dfs(int k)&#123;//k为盒子的编号，或可以理解为步数 if(k==r+1&amp;&amp;is_rise(a))&#123; for(int i=1;i&lt;=r;i++)&#123; cout&lt;&lt;a[i]&lt;&lt;&quot; &quot;; &#125; cout&lt;&lt;&quot;\\n&quot;; return; &#125; for(int i=1;i&lt;=n;i++)&#123;//注意只要一个for来表示其手上所拿的牌即可，不要用两个for，递归里面就包含了向下循环的方式，就是一遍一遍尝试放牌。 if(vis[i]==0)&#123; a[k]=i,vis[i]=1; dfs(k+1); vis[i]=0;//将牌子拿出来，此时就要把vis归回0 &#125; &#125;&#125;int main()&#123; cin&gt;&gt;n&gt;&gt;r; dfs(1);&#125; 这题不用暴力for循环做解，而是考虑用三个盒子装入数字，装入过的数字用1标记箱子被使用。 素数环 eg：输入8 输出4 1234567891011121314151617181920212223242526272829303132#include&lt;bits/stdc++.h&gt;using namespace std;int a[25],vis[25];int n,cnt;int isPrime(int x)&#123; if(x&lt;2)return 0; for(int i=2;i*i&lt;=x;i++)&#123; if(x%i==0)return 0; &#125; return 1;&#125;void dfs(int k)&#123; if(k==n+1 &amp;&amp; isPrime(a[1]+a[n]))&#123;//是否超出边界&amp;&amp;是否头尾相加是素数 cnt++; return; &#125; for(int i=2;i&lt;=n;i++)&#123; if(vis[i]==0 &amp;&amp; isPrime(i+a[k-1]))&#123;//vis用来看有没有用过这个数字 a[k]=i,vis[i]=1;//a用来保存数字 dfs(k+1); vis[i]=0;//当前的这个数字清除，再向下dfs &#125; &#125;&#125;int main()&#123; cin&gt;&gt;n; a[1]=1,vis[1]=1; dfs(2); cout&lt;&lt;cnt; &#125; 全排列问题 1234567891011121314151617181920212223#include &lt;bits/stdc++.h&gt;using namespace std;int n;int a[25],b[25]; //a用来存储数字，到时候输出就看a里面存的数字；b用来记录数字使用过没有，如果用了就用1表示，没用就是0；比如1 2 3，在存放第二个盒子的时候1已经用过了，故用b[i]==0来判断出可以用2，再把2放到盒子里void dfs(int k)&#123; //depth first search if(k==n+1)&#123; //k是指到第几个盒子了，如果k到了第n+1个虚无的盒子，就说明没盒子了要输出了 for(int i=1;i&lt;=n;i++)&#123; printf(&quot;%d &quot;,a[i]); &#125; printf(&quot;\\n&quot;); return; &#125; for(int i=1;i&lt;=n;i++)&#123;//i是指拿在你手上的牌的数字，没用0为了更好理解 if(b[i]==0)&#123;//看看这个牌用过没有，b数组用来看这个牌用过没有用的 a[k]=i,b[i]=1;//如果没有用过就把牌i放到第k个盒子里，用a[k]=i表示，再用b[i]=1表示这个牌用了 dfs(k+1);//上一步只放了一张牌，这一步就是看到第二个盒子，在这次i会发现b[1]=1，因此此时i会为2，并把2放到盒子里 b[i]=0;//就是把当前的这张牌拿出来，比如n=3时，它时在dpf(3)时先将i=3的拿出来，然后再退回上一个dpf(2)把i=2那出来，然后dpf(2)这段又会i+1变成3，此时又到dps(3)里，以此类推 &#125;&#125;&#125;int main()&#123; scanf(&quot;%d&quot;,&amp;n); dfs(1);&#125; 体积 12345678910111213141516171819202122232425262728#include &lt;bits/stdc++.h&gt;using namespace std;int n,cnt;int a[25],vis[1005];void dfs(int k,int sum)&#123; if(k==n+1)&#123;// cout&lt;&lt;sum&lt;&lt;&quot; &quot;; vis[sum]=1; return; &#125; dfs(k+1,sum+a[k]); dfs(k+1,sum);&#125;int main()&#123; cin&gt;&gt;n; for(int i=1;i&lt;=n;i++)&#123; cin&gt;&gt;a[i]; &#125; dfs(1,0); for(int i=1;i&lt;=1000;i++)&#123; if(vis[i])cnt++; &#125; cout&lt;&lt;cnt; return 0;&#125; 若把上面代码的注释删除则可以得到： 12331 3 48 4 5 1 7 3 4 0 6 由此可知上面深度搜索遍历的顺序是： 123456781+3+41+31+413+4340 以上手写笔记逐个分析dfs递归情况，方便理解两个dfs同时出现的状态。 相当于第一个dfs一个一个出栈，出栈一个数据就进入下一个栈再进行递取。","categories":[],"tags":[]},{"title":"爬取pixiv日前十图片并下载至本地","slug":"pixiv-crawler","date":"2023-09-11T04:21:25.000Z","updated":"2023-11-06T14:33:09.180Z","comments":true,"path":"2023/09/11/pixiv-crawler/","link":"","permalink":"https://bayeeaa.github.io/2023/09/11/pixiv-crawler/","excerpt":"","text":"爬取pixivic日前十图片实现目标：通过爬虫代码将pixivic日前十图片下载至本地并通过文件夹分类日期： 1.发送请求首先通过f12检查网站传输的包，发现ranks文件，查看其api所对应的响应，可以找到图片的url下载地址： 于是我们就可以向这个api发送请求： 1234import requestsurl = &quot;https://api.bbmang.me/ranks?page=1&amp;date=2023-09-07&amp;mode=day&amp;pageSize=30&quot;reponse = requests.get(url)print(reponse) #返回&lt;Response [200]&gt; 因为返回值为200，说明服务器已经同意请求，并通过reponse.text发现其返回值是标准的json文件，没有出现乱码等反爬现象，因此不用再加入请求头。 2.解析数据返回数据长这个样子： json使用这就是标准的json形式文件，但还不是一个字典(其实很想)，所以我们要通过response.json()指令将其变成一个字典，这样才可以通过字典与列表的操作方式提取数据。 我们可以看到里面的数据其实就是字典与列表的不断嵌套，所以只要一点点剥下去即可： 123response.json()[&quot;data&quot;][0][&#x27;imageUrls&#x27;][0][&#x27;original&#x27;]#返回为https://i.pximg.net/img-original/img/2023/09/06/00/46/45/111480211_p0.png#就是日排行第一的图 但是当你将网站复制进浏览器时你会发现打不开。 url拼接也许是因为这个url是p站之前的使用方法，这时通过与现在图片的打开地址进行比对可以看到其中可以进行暴力拼接 图中方框处就是相同的部分，因此我们可以通过列表的切片将其拼接，这样我们就可以得到可以用的图片下载url了。 3.下载图片至本地目前我们已经得到了今天top1的图片url，我们再通过request请求获取图片数据，然后就可以把图片下载到本地了： 12345img_url = &quot;https://acgpic.net/c/540x540_70/img-master&quot;+response.json()[&quot;data&quot;][0][&#x27;imageUrls&#x27;][0][&#x27;original&#x27;][32:-4]+&quot;_master1200.jpg&quot;img = requests.get(img_url)with open(&quot;./top1.jpg&quot;,&quot;wb&quot;) as f: #&quot;wb&quot;是以二进制写入 f.write(img.content) #content是指获取二进制内容 此时你已经可以看到top1的图片下载当前文件夹了。 同理，下载前十的图片只需要加入for循环就可以了，以下省略。 4.下载至指定文件夹这里就需要导入os包： 1234import ospath = &#x27;C:/Users/yyn19/Desktop/code/download_imges/2023-9-7&#x27; #这是我的绝对地址if(os.path.exists(path)==False): #判断是否存在该文件夹 os.mkdir(path) #创建该文件夹，文件夹名字为2023-9-7 path书写这其中的path是linux写法书写，为了方便我进行字符串拼接，这其中path也有三种写法： 第二条中的r是用来申明不是转义字符。 这样就可以创建一个文件夹了~ 然后只要再with open() as中加入path路径就可以将其下载到指定文件夹中了。 5.关于api中日期的变化因为每天api的地址都会更新，这里我就用了datetime包来获取时间信息，我发现api中的日期都会比我们现在的时间少3天，因此只要一下操作就能获取api中的时间日期： 123import datetimeday = datetime.date.today()api_day = day-datetime.timedelta(days=3) 然后只需要再拼接如url中就可以啦~ 完整代码123456789101112131415161718192021import requestsimport osimport datetimeday = datetime.date.today()api_day = day-datetime.timedelta(days=3)url = &quot;https://api.bbmang.me/ranks?page=1&amp;date=&quot;+str(api_day)+&quot;&amp;mode=day&amp;pageSize=302&quot;response = requests.get(url)date = url[40:50]path = &#x27;C:/Users/yyn19/Desktop/code/download_imges/&#x27;+date #linux写法if(os.path.exists(path)==False): #判断是否存在该文件夹 os.mkdir(path) #创建该文件夹for i in range(10): img_url = &quot;https://acgpic.net/c/540x540_70/img-master&quot;+response.json()[&quot;data&quot;][i][&#x27;imageUrls&#x27;][0][&#x27;original&#x27;][32:-4]+&quot;_master1200.jpg&quot; img = requests.get(img_url) with open(path+&quot;/top&quot;+str(i+1)+&quot;.jpg&quot;,&quot;wb&quot;) as f: f.write(img.content) print(&quot;正在保存top&quot;+str(i+1)+&quot;中...&quot;)","categories":[],"tags":[]},{"title":"爬虫 --- 以爬取笔趣阁小说为例","slug":"biquge","date":"2023-08-31T07:40:23.000Z","updated":"2023-09-10T14:03:51.640Z","comments":true,"path":"2023/08/31/biquge/","link":"","permalink":"https://bayeeaa.github.io/2023/08/31/biquge/","excerpt":"","text":"爬虫 — 以爬取笔趣阁小说为例1.发送请求123456import requestsurl = &quot;https://www.xzmncy.com/list/5418/2610707.html&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot;&#125;response = requests.get(url,headers) 这是requests请求，若返回response值为200，则表示请求成功 2.获取数据1response = requests.get(url,headers).text 可以通过以上方法返回得到的html文件内容，而文件中有很多标签在里面，不能直接获取想要的信息，所以需要数据解析 3.解析数据有以下几种途径：css、xpath、re正则表达 等等 让我们来看看分别用这三种方法怎么去解析到一个章节的标题 css123import parselselector = parsel.Selector(response)novel_title = selector.css(&quot;.bookname h1::text&quot;).get() 这种方法通过css选择器进行选择 xpath123import parselselector = parsel.Selector(response)novel_title = selector.xpath(&quot;//*[@class=&quot;bookname&quot;]/h1/text()&quot;).get() 注意text后面的() re12import renovel_title = re.findall(&quot;&lt;h1&gt;(.*?)&lt;/h1&gt;&quot;,response)[0] 这里是因为h1在html文件中只有一个，故我直接导入。获取的数据是一个列表，所以我后面做了个且切片来直接获取字符串 *注意：以上方法各有利弊，选择合适的方式来解析数据 4.保存数据12with open(&quot;file_name&quot;+&quot;.txt&quot;,mode=&quot;w&quot;,encoding=&quot;utf-8&quot;) as f: #w是写入但是覆盖，a是追加写入，写入文件末尾 wb是二进制写入f.write(novel_context) #写入文件 with open(download_path,mode&#x3D;””,encoding&#x3D;”utf-8”)中间的download_path可以写绝对路径 以上思路已经理清楚了，下面开始实践：爬取一章12345678910111213import parselimport requestsurl = &quot;https://www.xzmncy.com/list/5418/2610707.html&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot;&#125;response = requests.get(url,headers).textselector = parsel.Selector(response)novel_title = selector.css(&quot;.bookname h1::text&quot;).get() #css方法解析数据novel_context_list = selector.css(&quot;#htmlContent p::text&quot;).getall() novel_context = &quot;\\n&quot;.join(novel_context_list) 注意：join函数的使用： 123456a=[&quot;1&quot;,&quot;2&quot;,&quot;8&quot;,&quot;9&quot;]print(&quot; &quot;.join(a)) #输出1 2 8 9print(&quot;\\n&quot;.join(a)) #输出1(换行)2(换行)8(换行)9b=&#123;&quot;a&quot;:1,&quot;b&quot;:2&#125;print(&quot; &quot;.join(a)) #输出a b （注意seq不能是int整形） 爬取各章url12345678910111213import requestsimport reurl = &quot;https://www.xzmncy.com/list/18753/&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot; &#125;response = requests.get(url,headers).textnovel_name = re.findall(&quot;&lt;h1&gt;(.*?)&lt;/h1&gt;&quot;,response)[0]novel_info = re.findall(&#x27;&lt;dd&gt;&lt;a href=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&lt;/dd&gt;&#x27;,response)for novel_url_part,novel_title in novel_info: novel_url = &quot;https://www.xzmncy.com&quot;+novel_url_part[0:24] print(novel_url) print(novel_title) 在小说的列表页面我们可以发现每个标签对应的章节url，此时我们获取的数据是这样的： 我们就可以用re来解析到各个章节的url和title 完整代码123456789101112131415161718192021222324import requestsimport reimport parselurl = &quot;https://www.xzmncy.com/list/18753/&quot;headers = &#123; &quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0&quot; &#125;response = requests.get(url,headers).textnovel_name = re.findall(&quot;&lt;h1&gt;(.*?)&lt;/h1&gt;&quot;,response)[0]novel_info = re.findall(&#x27;&lt;dd&gt;&lt;a href=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&lt;/dd&gt;&#x27;,response)for novel_url_part,novel_title in novel_info: novel_url = &quot;https://www.xzmncy.com&quot;+novel_url_part[0:24] novel_response = requests.get(novel_url, headers).text selectors = parsel.Selector(novel_response) novel_context_list = selectors.css(&quot;#htmlContent p::text&quot;).getall() novel_context = &quot;\\n&quot;.join(novel_context_list) print(&quot;正在保存&quot;+novel_title) novel_title = &quot;*&quot; + novel_title with open(novel_name+&quot;.txt&quot;,mode=&quot;a&quot;) as f: f.write(novel_title) f.write(&quot;\\n&quot;) f.write(novel_context) f.write(&quot;\\n&quot;) f.write(&quot;\\n&quot;) 运行代码就可以看到当前的目录下出现一个txt文件，里面就是想要的小说啦~","categories":[],"tags":[]},{"title":"CSS选择器","slug":"css选择器","date":"2023-08-23T04:35:42.000Z","updated":"2023-08-24T07:48:46.105Z","comments":true,"path":"2023/08/23/css选择器/","link":"","permalink":"https://bayeeaa.github.io/2023/08/23/css%E9%80%89%E6%8B%A9%E5%99%A8/","excerpt":"","text":"引入方式引入方式有以下三种： 1.内嵌式 12345678910111213&lt;!-- 内嵌式 --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style&gt; colour&#123; colour:pink; &#125; &lt;/style&gt;&lt;/head&gt; 2.外联式 12345678910&lt;!-- 外联式 --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Document&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;./111.css&quot;&gt; &lt;!-- 111为引入文件名 --&gt;&lt;/head&gt; 3.行内式 1234567&lt;!-- 行内式 --&gt;&lt;body&gt; &lt;div class=&quot;colour&quot;&gt; abcd &lt;/div&gt; &lt;div style=&quot;color: aqua;font-size: large;&quot;&gt;abab&lt;/div&gt;&lt;/body&gt; 选择器一共有4种：标签选择器、类选择器、id选择器、通符选择器 注：一下选择器均是在style标签下的 1.标签123div&#123; color:blue;&#125; 所选类型与body中html的文本类型相符(如上文中，其下文body中应该是div) 2.类选择器123.color-choose&#123; color:blue;&#125; 其html调用方式为： 1&lt;div class=&quot;color-choose&quot;&gt; abab &lt;/div&gt; 3.id选择器123#color&#123; color:blue;&#125; 其html调用方式为： 1&lt;div id=&quot;color&quot;&gt; abab &lt;/div&gt; 注意：id只得调用一次 4.通符选择器12345*&#123; margin:0; padding:0;&#125;&lt;!-- 清除内外边距 --&gt; 对全局内容生效 选择器的选择1.后代 （后面所有代）问题如下 1234&lt;p&gt; abab &lt;/p&gt;&lt;div&gt; &lt;p&gt; 哈哈哈 &lt;/p&gt;&lt;/div&gt; 欲选择div中的p标签，而不是外部的p 以如下方法实现： 12345&lt;style&gt; div p &#123; color:blue; &#125;&lt;/style&gt; 2.子代 （后面一代）问题是要选中div后面的一代 123456&lt;div&gt; &lt;p&gt; dd &lt;/p&gt; &lt;p&gt; &lt;a href=&quot;#&quot;&gt; ddd &lt;/a&gt; &lt;/p&gt;&lt;/div&gt; 以如下方法实现： 123div&gt;p&#123; color:blue;&#125; 3.并集问题：想要让以下这些标签被选到 1234&lt;p&gt; p &lt;/p&gt;&lt;div&gt; div &lt;/div&gt;&lt;span&gt; span &lt;/span&gt;&lt;h1&gt; haha &lt;/h1&gt; 以下面方法实现： 123p,div,span,h1&#123; color:blue;&#125; 4.交集问题：只想要选中下面p中带class&#x3D;”c”的 1234&lt;div class=&quot;c&quot;&gt;abcd&lt;/div&gt;&lt;p class=&quot;c&quot;&gt;a&lt;/p&gt;&lt;div class=&quot;c&quot;&gt;d&lt;/div&gt;&lt;p class=&quot;c&quot;&gt;b&lt;/p&gt; 以下面方法实现： 123p.c&#123; color:blue;&#125; p是标签，c是类名（前面带个.的） 5.伪类问题：想要让鼠标悬停在如下超链接上能够变色 1&lt;a href=&quot;~~~&quot;&gt;传送&lt;/a&gt; 一下方法实现： 1234a:hover&#123; color:red; background-color:yellow;&#125;","categories":[],"tags":[]},{"title":"破壳啦","slug":"page1","date":"2023-08-21T11:25:42.000Z","updated":"2023-08-22T13:45:57.473Z","comments":true,"path":"2023/08/21/page1/","link":"","permalink":"https://bayeeaa.github.io/2023/08/21/page1/","excerpt":"","text":"终于创建好一个博客啦！","categories":[],"tags":[]}],"categories":[],"tags":[]}